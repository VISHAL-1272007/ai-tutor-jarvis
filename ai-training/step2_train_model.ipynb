{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc55191",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9839244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (takes 2-3 minutes)\n",
    "!pip install -q transformers datasets accelerate peft bitsandbytes trl huggingface_hub\n",
    "!pip install -q torch torchvision torchaudio\n",
    "\n",
    "print(\"‚úÖ Libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea4821",
   "metadata": {},
   "source": [
    "## üîë Step 2: Login to Hugging Face\n",
    "\n",
    "1. Go to https://huggingface.co and create FREE account\n",
    "2. Go to Settings ‚Üí Access Tokens ‚Üí Create new token\n",
    "3. Copy the token and paste below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Your Hugging Face token - Get from: https://huggingface.co/settings/tokens\n",
    "# IMPORTANT: Replace this with YOUR token when running in Colab!\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # ‚Üê Paste your token here\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "print(\"‚úÖ Logged in to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989e231",
   "metadata": {},
   "source": [
    "## üìä Step 3: Load Your Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load training data\n",
    "with open('jarvis_training_data.json', 'r', encoding='utf-8') as f:\n",
    "    training_data = json.load(f)\n",
    "\n",
    "print(f\"üìä Model Name: {training_data['model_name']}\")\n",
    "print(f\"üìä Total Examples: {training_data['total_examples']}\")\n",
    "print(f\"üìä Created By: {training_data['created_by']}\")\n",
    "print()\n",
    "print(\"üìã Sample Data:\")\n",
    "print(\"-\" * 50)\n",
    "sample = training_data['data'][0]\n",
    "print(f\"Question: {sample['instruction'][:100]}...\")\n",
    "print(f\"Answer: {sample['output'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813c3d5",
   "metadata": {},
   "source": [
    "## ü§ñ Step 4: Load Base Model\n",
    "\n",
    "We'll use **TinyLlama** for faster training (good for learning).\n",
    "Later you can upgrade to Llama 3 or Mistral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465224b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Check GPU\n",
    "print(f\"üñ•Ô∏è GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üñ•Ô∏è GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üñ•Ô∏è GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Model name (TinyLlama for faster training)\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# For better results later, use:\n",
    "# MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "print(f\"\\nüì• Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take 2-5 minutes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization config (reduces memory usage)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de353f2",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Prepare for Fine-Tuning (LoRA)\n",
    "\n",
    "**LoRA** = Low-Rank Adaptation\n",
    "- Trains only small parts of the model\n",
    "- Much faster and uses less memory\n",
    "- Results are almost as good as full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23dfea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                      # Rank (higher = more capacity)\n",
    "    lora_alpha=32,             # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Which layers to train\n",
    "    lora_dropout=0.05,         # Dropout for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "print(\"\\n‚úÖ Model ready for fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb205ad9",
   "metadata": {},
   "source": [
    "## üìù Step 6: Prepare Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40aa8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format data for training\"\"\"\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Response:\n",
    "{example['output']}\"\"\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Create dataset\n",
    "data = training_data['data']\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "print(f\"‚úÖ Dataset created with {len(dataset)} examples\")\n",
    "print()\n",
    "print(\"üìã Sample formatted prompt:\")\n",
    "print(\"-\" * 50)\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e116bdd4",
   "metadata": {},
   "source": [
    "## üöÄ Step 7: Train Your JARVIS AI!\n",
    "\n",
    "This is where the magic happens! ‚ú®\n",
    "\n",
    "Training time: ~10-30 minutes depending on data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4293d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Your Hugging Face username\n",
    "HF_USERNAME = \"aijarvis2025\"\n",
    "\n",
    "# Output model name - Your AI model!\n",
    "OUTPUT_MODEL = f\"{HF_USERNAME}/jarvis-edu-ai\"\n",
    "\n",
    "print(f\"üì§ Your model will be saved to: https://huggingface.co/{OUTPUT_MODEL}\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./jarvis-model\",\n",
    "    num_train_epochs=3,              # Number of times to go through data\n",
    "    per_device_train_batch_size=4,   # Samples per batch\n",
    "    gradient_accumulation_steps=4,   # Accumulate gradients\n",
    "    warmup_steps=10,                 # Warmup learning rate\n",
    "    learning_rate=2e-4,              # Learning rate\n",
    "    bf16=True,                       # Use bfloat16 precision (better for newer GPUs)\n",
    "    logging_steps=10,                # Log every N steps\n",
    "    save_strategy=\"epoch\",           # Save after each epoch\n",
    "    optim=\"paged_adamw_8bit\",        # Optimizer\n",
    "    report_to=\"none\"                 # Don't report to wandb\n",
    ")\n",
    "\n",
    "# Create trainer (minimal version for newest TRL API)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "print(\"üöÄ Ready to train!\")\n",
    "print(\"Run the next cell to start training...\")\n",
    "print(\"This will take 10-30 minutes. Go grab a coffee! ‚òï\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model!\n",
    "trainer.train()\n",
    "\n",
    "print()\n",
    "print(\"üéâ Training complete!\")\n",
    "print(\"Your JARVIS AI has learned from your data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a356263",
   "metadata": {},
   "source": [
    "## üíæ Step 8: Save Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d23e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally first\n",
    "trainer.save_model(\"./jarvis-final\")\n",
    "tokenizer.save_pretrained(\"./jarvis-final\")\n",
    "\n",
    "print(\"‚úÖ Model saved locally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c974376",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Step 9: Upload to Hugging Face (FREE Hosting!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c5fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Create model card\n",
    "model_card = f\"\"\"---\n",
    "license: apache-2.0\n",
    "language:\n",
    "- en\n",
    "tags:\n",
    "- education\n",
    "- jarvis\n",
    "- ai-tutor\n",
    "---\n",
    "\n",
    "# üß† JARVIS Educational AI\n",
    "\n",
    "## Created by: Vishal\n",
    "\n",
    "This is a fine-tuned language model specialized for educational assistance.\n",
    "\n",
    "### What JARVIS Can Do:\n",
    "- üíª Programming help (Python, JavaScript, Java, etc.)\n",
    "- üìä Data Structures & Algorithms explanations\n",
    "- üåê Web Development guidance\n",
    "- üíº Career advice for IT freshers\n",
    "- üìö Study tips and learning strategies\n",
    "\n",
    "### Usage:\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"{OUTPUT_MODEL}\")\n",
    "response = pipe(\"What is Python?\")[0]['generated_text']\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### Training:\n",
    "- Base Model: TinyLlama-1.1B-Chat\n",
    "- Method: LoRA Fine-tuning\n",
    "- Data: Custom educational Q&A dataset\n",
    "\n",
    "Built for 30,000+ students! üéì\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(\"./jarvis-final/README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"‚úÖ Model card created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to Hugging Face\n",
    "print(f\"üì§ Uploading to Hugging Face: {OUTPUT_MODEL}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "model.push_to_hub(OUTPUT_MODEL)\n",
    "tokenizer.push_to_hub(OUTPUT_MODEL)\n",
    "\n",
    "print()\n",
    "print(\"üéâüéâüéâ SUCCESS! üéâüéâüéâ\")\n",
    "print()\n",
    "print(f\"Your JARVIS AI is now live at:\")\n",
    "print(f\"https://huggingface.co/{OUTPUT_MODEL}\")\n",
    "print()\n",
    "print(\"You can now use this model in your website!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5965dde0",
   "metadata": {},
   "source": [
    "## üß™ Step 10: Test Your JARVIS AI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb48b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_jarvis(question):\n",
    "    \"\"\"Ask JARVIS a question\"\"\"\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "# Test questions\n",
    "print(\"üß™ Testing JARVIS AI...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain loops in programming\",\n",
    "    \"How to get a job as fresher?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n‚ùì Question: {q}\")\n",
    "    print(f\"ü§ñ JARVIS: {ask_jarvis(q)[:500]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a8b899",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ Congratulations!\n",
    "\n",
    "## You just built your own AI model!\n",
    "\n",
    "### What You Achieved:\n",
    "- ‚úÖ Downloaded a pre-trained language model\n",
    "- ‚úÖ Fine-tuned it on YOUR educational data\n",
    "- ‚úÖ Created YOUR OWN JARVIS AI\n",
    "- ‚úÖ Uploaded to Hugging Face (FREE hosting)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Add More Training Data** - More data = Better AI\n",
    "2. **Upgrade Base Model** - Try Llama 3 or Mistral\n",
    "3. **Connect to Website** - Use your model in JARVIS\n",
    "4. **Add RAG System** - Give JARVIS your knowledge base\n",
    "\n",
    "---\n",
    "\n",
    "### You're now an AI Developer! üß†üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
